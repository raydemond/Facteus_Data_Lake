{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Facteus Data Lake\n",
    "\n",
    "## Project Summary\n",
    "* The project helps Facteus to build a data lake hosted on AWS S3 to understand how COVID-19 affects payments transactions.\n",
    "* I built an ETL pipeline to extract raw data from S3, process them using Spark, and transform into dimension tables for analytic purposes.\n",
    "\n",
    "The project follows the following process:\n",
    "* Scope the Project and Gather Data\n",
    "* Explore and Assess the Data\n",
    "* Define the Data Model\n",
    "* Run ETL to Model the Data\n",
    "* Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import all the necessary packages\n",
    "import os\n",
    "import configparser\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1. Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "* The project builds an ETL pipeline to move millions of payment transaction records after COVID-19 outbreak, COVID cases tracking data and other demographic datasets to a data lake hosted on S3. \n",
    "* The data lake can generate insights of how the pandemic change people's payment behavior, how business sectors are affected, and how different demographic and geographic groups react to the pandemic.  \n",
    "* I would use PySpark to extract data from AWS S3, process them and load them into dimension tables, then store the tables back to S3.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "* **Daily Spending Data**: The dataset comes from Facteus, a provider of financial data business intelligence. It covers payments transactions from 2020/04/17 to 2020/07/07, including payment date, merchant information, card holder zip code and payment details.\n",
    "* **Covid County Data**: It is open sourced data tracking daily total Covid-19 tests, cases and deaths by county.\n",
    "* **U.S. City Demographic Data**  This data comes from US Census Bureau's 2015 American Community Survey. It includes household details in the cities.\n",
    "* **Zip_City Data**: The data comes from Simplemaps. It connects Zip Code with city and county.\n",
    "* **ZIP Code Crosswalk Data**: The data comes from HUD’s Office of Policy Development and Research. It relates ZIP codes to county FIPS, including county composition details.\n",
    "* **Merchant Category Code Data** The data comes from Visa Merchant Data Standards Manual. It relates merchant category codes to merchant names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('credentials.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.1 Extract Daily Spending Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fix the schema\n",
    "paymentSchema = R([\n",
    "    Fld('Date',Date()),\n",
    "    Fld('MCC',Long()),\n",
    "    Fld('Zip_code',Long()),\n",
    "    Fld('Num_card',Long()),\n",
    "    Fld('Num_trnsc',Long()),\n",
    "    Fld('Total_spend',Dbl())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the csv file to a Spark DataFrame\n",
    "df_payment = spark.read.csv(\"s3a://transaction-dataset/daily-spend/*.csv\",schema = paymentSchema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+--------+---------+-----------+\n",
      "|      Date| MCC|Zip_code|Num_card|Num_trnsc|Total_spend|\n",
      "+----------+----+--------+--------+---------+-----------+\n",
      "|2020-06-16|4814|   55414|       7|        7|      853.6|\n",
      "|2020-06-16|5655|   55414|       1|        1|     161.94|\n",
      "|2020-06-16|5651|   55414|       4|        4|     242.25|\n",
      "|2020-06-16|5399|   55414|       1|        1|     127.02|\n",
      "|2020-06-16|5814|   55414|      59|       84|     645.15|\n",
      "+----------+----+--------+--------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_payment.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.2 Extract Covid County Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fix the schema\n",
    "covidSchema = R([\n",
    "    Fld('Date',Date()),\n",
    "    Fld('County',Long()),\n",
    "    Fld('Name',Str()),\n",
    "    Fld('Total_tests',Long()),\n",
    "    Fld('Total_deaths',Dbl()),\n",
    "    Fld('Total_cases',Dbl())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the csv file to a Spark DataFrame\n",
    "df_covid = spark.read.csv(\"s3a://transaction-dataset/covid-cases.csv\",schema = covidSchema,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>County</th>\n",
       "      <th>Name</th>\n",
       "      <th>Total_tests</th>\n",
       "      <th>Total_deaths</th>\n",
       "      <th>Total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  County     Name  Total_tests  Total_deaths  Total_cases\n",
       "0  2020-01-21    1001  Autauga          NaN           0.0          0.0\n",
       "1  2020-01-22    1001  Autauga          NaN           0.0          0.0\n",
       "2  2020-01-23    1001  Autauga          NaN           0.0          0.0\n",
       "3  2020-01-24    1001  Autauga          NaN           0.0          0.0\n",
       "4  2020-01-25    1001  Autauga          NaN           0.0          0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.3 Extract U.S. City Demographic Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fix the schema\n",
    "citySchema = R([\n",
    "    Fld('City',Str()),\n",
    "    Fld('State',Str()),\n",
    "    Fld('Median_age',Dbl()),\n",
    "    Fld('Male_pop',Long()),\n",
    "    Fld('Female_pop',Long()),\n",
    "    Fld('Total_pop',Long()),\n",
    "    Fld('Num_vet',Long()),\n",
    "    Fld('Foreign_born',Long()),\n",
    "    Fld('Avg_size',Dbl()),\n",
    "    Fld('State_code',Str()),\n",
    "    Fld('Race',Str()),\n",
    "    Fld('Count',Long())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the csv file to a Spark DataFrame \n",
    "df_city = spark.read.option(\"delimiter\", \";\").csv(\"s3a://transaction-dataset/us-cities-demographics.csv\", schema = citySchema, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median_age</th>\n",
       "      <th>Male_pop</th>\n",
       "      <th>Female_pop</th>\n",
       "      <th>Total_pop</th>\n",
       "      <th>Num_vet</th>\n",
       "      <th>Foreign_born</th>\n",
       "      <th>Avg_size</th>\n",
       "      <th>State_code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median_age  Male_pop  Female_pop  \\\n",
       "0     Silver Spring       Maryland        33.8   40601.0     41862.0   \n",
       "1            Quincy  Massachusetts        41.0   44129.0     49500.0   \n",
       "2            Hoover        Alabama        38.5   38040.0     46799.0   \n",
       "3  Rancho Cucamonga     California        34.5   88127.0     87105.0   \n",
       "4            Newark     New Jersey        34.6  138040.0    143873.0   \n",
       "\n",
       "   Total_pop  Num_vet  Foreign_born  Avg_size State_code  \\\n",
       "0      82463   1562.0       30908.0      2.60         MD   \n",
       "1      93629   4147.0       32935.0      2.39         MA   \n",
       "2      84839   4819.0        8229.0      2.58         AL   \n",
       "3     175232   5821.0       33878.0      3.18         CA   \n",
       "4     281913   5829.0       86253.0      2.73         NJ   \n",
       "\n",
       "                        Race  Count  \n",
       "0         Hispanic or Latino  25924  \n",
       "1                      White  58723  \n",
       "2                      Asian   4759  \n",
       "3  Black or African-American  24437  \n",
       "4                      White  76402  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.4 Extract City-Zips Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the csv file to a Spark DataFrame\n",
    "df_cityzip = spark.read.csv(\"s3a://transaction-dataset/uszips.csv\",header = True)\n",
    "\n",
    "# change zip type to LongType\n",
    "df_cityzip = df_cityzip.withColumn('zip',col('zip').cast(Long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>city</th>\n",
       "      <th>state_id</th>\n",
       "      <th>state_name</th>\n",
       "      <th>zcta</th>\n",
       "      <th>parent_zcta</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>county_fips</th>\n",
       "      <th>county_name</th>\n",
       "      <th>county_weights</th>\n",
       "      <th>county_names_all</th>\n",
       "      <th>county_fips_all</th>\n",
       "      <th>imprecise</th>\n",
       "      <th>military</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>18.18004</td>\n",
       "      <td>-66.75218</td>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>None</td>\n",
       "      <td>17242</td>\n",
       "      <td>111.4</td>\n",
       "      <td>72001</td>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>{'72001':99.43,'72141':0.57}</td>\n",
       "      <td>Adjuntas|Utuado</td>\n",
       "      <td>72001|72141</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>America/Puerto_Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>18.36073</td>\n",
       "      <td>-67.17517</td>\n",
       "      <td>Aguada</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>None</td>\n",
       "      <td>38442</td>\n",
       "      <td>523.5</td>\n",
       "      <td>72003</td>\n",
       "      <td>Aguada</td>\n",
       "      <td>{'72003':100}</td>\n",
       "      <td>Aguada</td>\n",
       "      <td>72003</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>America/Puerto_Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>18.45439</td>\n",
       "      <td>-67.12202</td>\n",
       "      <td>Aguadilla</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>None</td>\n",
       "      <td>48814</td>\n",
       "      <td>667.9</td>\n",
       "      <td>72005</td>\n",
       "      <td>Aguadilla</td>\n",
       "      <td>{'72005':100}</td>\n",
       "      <td>Aguadilla</td>\n",
       "      <td>72005</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>America/Puerto_Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>18.16724</td>\n",
       "      <td>-66.93828</td>\n",
       "      <td>Maricao</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>None</td>\n",
       "      <td>6437</td>\n",
       "      <td>60.4</td>\n",
       "      <td>72093</td>\n",
       "      <td>Maricao</td>\n",
       "      <td>{'72093':94.88,'72121':1.35,'72153':3.78}</td>\n",
       "      <td>Maricao|Yauco|Sabana Grande</td>\n",
       "      <td>72093|72153|72121</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>America/Puerto_Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>18.29032</td>\n",
       "      <td>-67.12243</td>\n",
       "      <td>Anasco</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>None</td>\n",
       "      <td>27073</td>\n",
       "      <td>312.0</td>\n",
       "      <td>72011</td>\n",
       "      <td>Añasco</td>\n",
       "      <td>{'72003':0.55,'72011':99.45}</td>\n",
       "      <td>Añasco|Aguada</td>\n",
       "      <td>72011|72003</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>America/Puerto_Rico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zip       lat        lng       city state_id   state_name  zcta  \\\n",
       "0  601  18.18004  -66.75218   Adjuntas       PR  Puerto Rico  TRUE   \n",
       "1  602  18.36073  -67.17517     Aguada       PR  Puerto Rico  TRUE   \n",
       "2  603  18.45439  -67.12202  Aguadilla       PR  Puerto Rico  TRUE   \n",
       "3  606  18.16724  -66.93828    Maricao       PR  Puerto Rico  TRUE   \n",
       "4  610  18.29032  -67.12243     Anasco       PR  Puerto Rico  TRUE   \n",
       "\n",
       "  parent_zcta population density county_fips county_name  \\\n",
       "0        None      17242   111.4       72001    Adjuntas   \n",
       "1        None      38442   523.5       72003      Aguada   \n",
       "2        None      48814   667.9       72005   Aguadilla   \n",
       "3        None       6437    60.4       72093     Maricao   \n",
       "4        None      27073   312.0       72011      Añasco   \n",
       "\n",
       "                              county_weights             county_names_all  \\\n",
       "0               {'72001':99.43,'72141':0.57}              Adjuntas|Utuado   \n",
       "1                              {'72003':100}                       Aguada   \n",
       "2                              {'72005':100}                    Aguadilla   \n",
       "3  {'72093':94.88,'72121':1.35,'72153':3.78}  Maricao|Yauco|Sabana Grande   \n",
       "4               {'72003':0.55,'72011':99.45}                Añasco|Aguada   \n",
       "\n",
       "     county_fips_all imprecise military             timezone  \n",
       "0        72001|72141     FALSE    FALSE  America/Puerto_Rico  \n",
       "1              72003     FALSE    FALSE  America/Puerto_Rico  \n",
       "2              72005     FALSE    FALSE  America/Puerto_Rico  \n",
       "3  72093|72153|72121     FALSE    FALSE  America/Puerto_Rico  \n",
       "4        72011|72003     FALSE    FALSE  America/Puerto_Rico  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cityzip.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.5 Extract ZIP Code Crosswalk Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countySchema = R([\n",
    "    Fld('Zipcode',Long()),\n",
    "    Fld('County',Long()),\n",
    "    Fld('Res_ratio',Dbl()),\n",
    "    Fld('Bus_ratio',Dbl()),\n",
    "    Fld('Oth_ratio',Dbl()),\n",
    "    Fld('Tot_ratio',Dbl())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the csv file to a Spark DataFrame\n",
    "df_county = spark.read.csv(\"s3a://transaction-dataset/zipcode-county.csv\",schema = countySchema,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>County</th>\n",
       "      <th>Res_ratio</th>\n",
       "      <th>Bus_ratio</th>\n",
       "      <th>Oth_ratio</th>\n",
       "      <th>Tot_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>36103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601</td>\n",
       "      <td>72113</td>\n",
       "      <td>0.160758</td>\n",
       "      <td>0.199017</td>\n",
       "      <td>0.128834</td>\n",
       "      <td>0.162397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>601</td>\n",
       "      <td>72001</td>\n",
       "      <td>0.839242</td>\n",
       "      <td>0.800983</td>\n",
       "      <td>0.871166</td>\n",
       "      <td>0.837603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>602</td>\n",
       "      <td>72003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>602</td>\n",
       "      <td>72005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Zipcode  County  Res_ratio  Bus_ratio  Oth_ratio  Tot_ratio\n",
       "0      501   36103   0.000000   1.000000   0.000000   1.000000\n",
       "1      601   72113   0.160758   0.199017   0.128834   0.162397\n",
       "2      601   72001   0.839242   0.800983   0.871166   0.837603\n",
       "3      602   72003   1.000000   0.998801   1.000000   0.999919\n",
       "4      602   72005   0.000000   0.001199   0.000000   0.000081"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_county.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.6 Extract Merchant Category Code Data from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "mccSchema = R([\n",
    "    Fld('MCC',Long()),\n",
    "    Fld('Name',Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the csv file to a Spark DataFrame\n",
    "df_mcc = spark.read.csv(\"s3a://transaction-dataset/MCC_List.csv\",schema = mccSchema,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCC</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>742.0</td>\n",
       "      <td>Veterinary Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>763.0</td>\n",
       "      <td>Agricultural Cooperatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>780.0</td>\n",
       "      <td>Landscaping and Horticultural Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1520.0</td>\n",
       "      <td>General Contractor/Residential Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1711.0</td>\n",
       "      <td>Heating, Plumbing, Air Conditioning Contractors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MCC                                             Name\n",
       "0   742.0                              Veterinary Services\n",
       "1   763.0                        Agricultural Cooperatives\n",
       "2   780.0           Landscaping and Horticultural Services\n",
       "3  1520.0          General Contractor/Residential Building\n",
       "4  1711.0  Heating, Plumbing, Air Conditioning Contractors"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcc.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "#### Cleaning the Data\n",
    "Remove duplicate rows and rows with missing values if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1 Clean Daily Spending Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19509232"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check Daily Spending Data\n",
    "df_payment.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19442575"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_payment.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19422738"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_payment.dropna(how = \"any\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop duplicate rows and missing values\n",
    "df_payment = df_payment.dropDuplicates().dropna(how = \"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2 Clean Covid County Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check Covid County Data\n",
    "df_covid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048438"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.dropna(how = \"any\", subset = ['Date','County','Total_deaths','Total_cases']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-----------+------------+-----------+\n",
      "|      Date|County|      Name|Total_tests|Total_deaths|Total_cases|\n",
      "+----------+------+----------+-----------+------------+-----------+\n",
      "|2021-01-17| 42083|    McKean|       null|        null|       null|\n",
      "|2021-01-17| 44001|   Bristol|      28896|        null|       null|\n",
      "|2021-01-17| 44003|      Kent|      81180|        null|       null|\n",
      "|2021-01-17| 44005|   Newport|      43252|        null|       null|\n",
      "|2021-01-17| 44007|Providence|     338209|        null|       null|\n",
      "|2021-01-17| 44009|Washington|      62111|        null|       null|\n",
      "+----------+------+----------+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the data \n",
    "df_covid.createOrReplaceTempView('table')\n",
    "spark.sql('SELECT * FROM table WHERE Total_deaths IS NULL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop rows with null total_deaths and total cases\n",
    "df_covid = df_covid.dropna(how = \"any\", subset = ['Date','County','Total_deaths','Total_cases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.3 Clean U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check U.S. City Demographic Data\n",
    "df_city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.dropna(how = \"any\",subset = ['City']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.4 Clean City-Zips Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33097"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check City-Zips Data\n",
    "df_cityzip.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33097"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cityzip.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33097"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cityzip.dropna(how = \"any\",subset = ['zip','city']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.5 Clean ZIP Code Crosswalk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54197"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check ZIP Code Crosswalk Data\n",
    "df_county.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54197"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_county.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54197"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_county.dropna(how = \"any\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.6 Clean Merchant Category Code Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check Merchant Category Code Data\n",
    "df_mcc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcc.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcc.dropna(how = \"any\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| MCC|                Name|\n",
      "+----+--------------------+\n",
      "|null|Visa - Non-Financ...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the null value\n",
    "df_mcc.filter(df_mcc.MCC.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_mcc = df_mcc.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3. Define the Data Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.1 Conceptual Data Model\n",
    "* The data lake is built on the snowflake schema. \n",
    "* The centralized fact table is the payment table.\n",
    "* The snowflake schema normalizes dimensions into multiple related tables since each dimension (covid, city demographics) contain more data than a single dimension table. \n",
    "* It also decreases the space to store the data and the number of places where it needs to be updated if the data changes.\n",
    "\n",
    "The ER diagram is shown below\n",
    "![Database ER diagram](./ER_Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Data Pipelines\n",
    "1. Create tables by selecting columns from Spark DataFrame\n",
    "2. Store tables in parquet files to AWS S3 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. Run Pipelines to Model the Data\n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_path = 's3a://datalake-payment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1.1 Write payment table to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create payment table\n",
    "payment_table = df_payment.select('Date',\n",
    "                                  'MCC',\n",
    "                                  'Zip_code',\n",
    "                                  'Num_card',\n",
    "                                  'Num_trnsc',\n",
    "                                  'Total_spend').withColumn('id', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write payment table to the parquet file partitioned by date\n",
    "payment_table.write.parquet(os.path.join(output_path,'payment.parquet'),'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1.2 Write covid table to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create covid table\n",
    "covid_table = df_covid.select('Date',\n",
    "                              'County',\n",
    "                              'Name',\n",
    "                              'Total_tests',\n",
    "                              'Total_deaths',\n",
    "                              'Total_cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write covid table to the parquet file partitioned by date\n",
    "covid_table.write.parquet(os.path.join(output_path,'covid.parquet'),'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1.3 Write city table to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create city table\n",
    "city_table = df_city.select('City',\n",
    "                            'State',\n",
    "                            'State_code',\n",
    "                            'Total_pop',\n",
    "                            'Male_pop',\n",
    "                            'Female_pop',\n",
    "                            'Avg_size',\n",
    "                            'Median_age',\n",
    "                            'Race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write city table to the parquet file partitioned by date\n",
    "city_table.write.parquet(os.path.join(output_path,'city.parquet'),'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1.4 Write zip_city table to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create city table\n",
    "zip_city_table = df_cityzip.selectExpr('zip AS Zip_code',\n",
    "                            'City',\n",
    "                            'State_name AS State',\n",
    "                            'State_id AS State_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "zip_city_table.write.parquet(os.path.join(output_path,'zip_city.parquet'),'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1.5 Write zip_county table to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "zip_county_table = df_county.selectExpr('Zipcode AS Zip_code',\n",
    "                                        'County',\n",
    "                                        'Res_ratio',\n",
    "                                        'Bus_ratio',\n",
    "                                        'Oth_ratio',\n",
    "                                        'Tot_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "zip_county_table.write.parquet(os.path.join(output_path,'zip_county.parquet'),'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1.6 Write mcc table to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "mcc_table = df_mcc.select('MCC',\n",
    "                          'Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "mcc_table.write.option(\"mergeSchema\", \"false\").parquet(os.path.join(output_path,'mcc.parquet'),'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the function for quality check\n",
    "def load_table(table):\n",
    "    \n",
    "    # load table\n",
    "    table_path = os.path.join(output_path, (table + '.parquet'))\n",
    "    df_table = spark.read.parquet(table_path)\n",
    "    df_table.createOrReplaceTempView(table)\n",
    "    \n",
    "    # check number of rows in the table\n",
    "    print(f\"Number of rows in the table {table}: {df_table.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.1 Check payment table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the table payment: 19422738\n"
     ]
    }
   ],
   "source": [
    "load_table('payment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|       Date|     date|   null|\n",
      "|        MCC|   bigint|   null|\n",
      "|   Zip_code|   bigint|   null|\n",
      "|   Num_card|   bigint|   null|\n",
      "|  Num_trnsc|   bigint|   null|\n",
      "|Total_spend|   double|   null|\n",
      "|         id|   bigint|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# integrity check\n",
    "spark.sql('DESCRIBE payment').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SpendperCard|\n",
      "+------------+\n",
      "|   73111.788|\n",
      "|    58710.84|\n",
      "|    33046.42|\n",
      "|     31826.3|\n",
      "|    25469.77|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unit test\n",
    "spark.sql('''SELECT Total_spend/Num_card AS SpendperCard \n",
    "             FROM payment\n",
    "             ORDER BY 1 DESC\n",
    "             LIMIT 5''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.2 Check covid table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the table covid: 1048438\n"
     ]
    }
   ],
   "source": [
    "load_table('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|        Date|     date|   null|\n",
      "|      County|   bigint|   null|\n",
      "|        Name|   string|   null|\n",
      "| Total_tests|   bigint|   null|\n",
      "|Total_deaths|   double|   null|\n",
      "| Total_cases|   double|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# integrity check\n",
    "spark.sql('DESCRIBE covid').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|       Name|Total_cases|\n",
      "+-----------+-----------+\n",
      "|Los Angeles|   105291.0|\n",
      "|     Queens|    65455.0|\n",
      "|      Kings|    59742.0|\n",
      "|   Maricopa|    52266.0|\n",
      "|      Bronx|    47651.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unit check\n",
    "spark.sql('''\n",
    "          SELECT Name, Total_cases\n",
    "          FROM covid\n",
    "          WHERE Date = \"2020-07-01\"\n",
    "          ORDER BY 2 DESC\n",
    "          LIMIT 5''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.3 Check city table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the table city: 2891\n"
     ]
    }
   ],
   "source": [
    "load_table('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|      City|   string|   null|\n",
      "|     State|   string|   null|\n",
      "|State_code|   string|   null|\n",
      "| Total_pop|   bigint|   null|\n",
      "|  Male_pop|   bigint|   null|\n",
      "|Female_pop|   bigint|   null|\n",
      "|  Avg_size|   double|   null|\n",
      "|Median_age|   double|   null|\n",
      "|      Race|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# integrity check\n",
    "spark.sql('DESCRIBE city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|        City|       State|Total_pop|\n",
      "+------------+------------+---------+\n",
      "|    New York|    New York|  8550405|\n",
      "| Los Angeles|  California|  3971896|\n",
      "|     Chicago|    Illinois|  2720556|\n",
      "|     Houston|       Texas|  2298628|\n",
      "|Philadelphia|Pennsylvania|  1567442|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unit check\n",
    "spark.sql('''\n",
    "            SELECT DISTINCT City,State, Total_pop\n",
    "            FROM city\n",
    "            ORDER BY 3 DESC\n",
    "            LIMIT 5\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.4 Check zip_city table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the table zip_city: 33097\n"
     ]
    }
   ],
   "source": [
    "load_table('zip_city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|  Zip_code|   bigint|   null|\n",
      "|      City|   string|   null|\n",
      "|     State|   string|   null|\n",
      "|State_code|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# integrity check\n",
    "spark.sql('DESCRIBE zip_city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+----------+\n",
      "|Zip_code|     City|      State|State_code|\n",
      "+--------+---------+-----------+----------+\n",
      "|     601| Adjuntas|Puerto Rico|        PR|\n",
      "|     602|   Aguada|Puerto Rico|        PR|\n",
      "|     603|Aguadilla|Puerto Rico|        PR|\n",
      "|     606|  Maricao|Puerto Rico|        PR|\n",
      "|     610|   Anasco|Puerto Rico|        PR|\n",
      "+--------+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unit check\n",
    "spark.sql('''\n",
    "            SELECT *\n",
    "            FROM zip_city\n",
    "            LIMIT 5\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.5 Check zip_county table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the table zip_county: 54197\n"
     ]
    }
   ],
   "source": [
    "load_table('zip_county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "| Zip_code|   bigint|   null|\n",
      "|   County|   bigint|   null|\n",
      "|Res_ratio|   double|   null|\n",
      "|Bus_ratio|   double|   null|\n",
      "|Oth_ratio|   double|   null|\n",
      "|Tot_ratio|   double|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# integrity check\n",
    "spark.sql('DESCRIBE zip_county').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-----------+-----------+-----------+\n",
      "|Zip_code|County|  Res_ratio|  Bus_ratio|  Oth_ratio|  Tot_ratio|\n",
      "+--------+------+-----------+-----------+-----------+-----------+\n",
      "|     501| 36103|        0.0|        1.0|        0.0|        1.0|\n",
      "|     601| 72113|0.160757734|0.199017199|0.128834356|0.162397217|\n",
      "|     601| 72001|0.839242266|0.800982801|0.871165644|0.837602783|\n",
      "|     602| 72003|        1.0|0.998800959|        1.0|0.999919368|\n",
      "|     602| 72005|        0.0|0.001199041|        0.0|  8.0632E-5|\n",
      "+--------+------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unit check\n",
    "spark.sql('''\n",
    "            SELECT *\n",
    "            FROM zip_county\n",
    "            LIMIT 5\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2.5 Check mcc table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the table mcc: 880\n"
     ]
    }
   ],
   "source": [
    "load_table('mcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|     MCC|   bigint|   null|\n",
      "|    Name|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# integrity check\n",
    "spark.sql('DESCRIBE mcc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| MCC|                Name|\n",
      "+----+--------------------+\n",
      "| 742| Veterinary Services|\n",
      "| 763|Agricultural Coop...|\n",
      "| 780|Landscaping and H...|\n",
      "|1520|General Contracto...|\n",
      "|1711|Heating, Plumbing...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unit check\n",
    "spark.sql('''\n",
    "            SELECT *\n",
    "            FROM mcc\n",
    "            LIMIT 5\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "##### payment table\n",
    "* id: automatically generated identification number\n",
    "* Date: transaction date\n",
    "* MCC: Visa Merchant Category Code\n",
    "* Zip_code: the cardholder’s residential zip code\n",
    "* Num_card: number of unique cards involved in transactions on the Date at merchants with that MCC\n",
    "* Num_trnsc: number of unique transactions on the Date at merchants with that MCC\n",
    "* Total_spend: total amount spent on the Date at merchants with that MCC\n",
    "\n",
    "##### covid table\n",
    "* Date: counting date\n",
    "* County: county FIPS \n",
    "* Name: county name\n",
    "* Total_tests: total covid tests\n",
    "* Total_deaths: total deaths caused by covid\n",
    "* Total_cases: total confirmed covid cases\n",
    "\n",
    "##### city table\n",
    "* City: city name\n",
    "* State: state name\n",
    "* State_code: abbreviation code for the state\n",
    "* Total_pop: total population\n",
    "* Male_pop: male population\n",
    "* Female_pop: female population\n",
    "* Avg_size: average family size\n",
    "* Median_age: median age\n",
    "* Race: race composition\n",
    "\n",
    "##### zip_city\n",
    "* Zip_code: zip code\n",
    "* City: city name\n",
    "* State: state name\n",
    "* State_code: abbreviation code for the state\n",
    "\n",
    "##### zip_county\n",
    "* Zip_code: zip code\n",
    "* County: county name\n",
    "* Res_ratio: the ratio of residential addresses in the zip\n",
    "* Bus_ratio: The ratio of business addresses in the zip\n",
    "* Oth_ratio: The ratio of other addresses in the zip\n",
    "* Tot_ratio: The ratio of total addresses in the zip \n",
    "\n",
    "##### mcc\n",
    "* MCC: Visa Merchant Category Code\n",
    "* Name: merchant name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5. Project Write Up\n",
    "**1. Why Data Lake?**\n",
    "* The key data is payments transactions. But what insights we want to drive is undefined. I add covid data into the data lake to examine how covid change people's consumption pattern. We can also put other data into data lake to drive other insights.\n",
    "\n",
    "**2. Why PySpark?**\n",
    "* Spark is much faster than Hadoop for large scale data processing and more user friendly.\n",
    "\n",
    "**3. How often the data should be updated?**\n",
    "* The data should be updated weekly. Allow a week for consumption pattern changes.\n",
    "\n",
    "**4. What if the data was increased by 100x?**\n",
    "* Spark can handle it with its better scalability and overall faster runtimes.\n",
    "\n",
    "**5. What if the data populates a dashboard that must be updated on a daily basis by 7am every day?**\n",
    "* Update the tables stored in the data lake daily using Append or Overwrite Mode by spark.write\n",
    "\n",
    "**6. What if the database needed to be accessed by 100+ people?**\n",
    "* The more people accessing the database the more CPU resources we need to get a fast experience. By using a distributed database we can improve the replications and partitioning to get faster query results for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
